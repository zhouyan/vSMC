\chapter{Resampling}
\label{chap:Resampling}

\section{Builtin algorithms}
\label{sec:Builtin algorithms}

The library supports resampling in a more general way than the algorithm
described in chapter~\ref{chap:Sequential Monte Carlo}. Recall that, given a
particle system $\{W^{(i)},X^{(i)}\}_{i=1}^N$, a new system $\{\bar{W}^{(i)},
\bar{X}^{(i)}\}_{i=1}^M$ is generated. Regardless of other statistical
properties, in practice, such an algorithm can be decomposed into three steps.
First, a vector of replication numbers $\{r_i\}_{i=1}^N$ is generated such that
$\sum_{i=1}^N r_i = M$, and $0 \le r_i \le M$ for $i=1,\dots,N$. Then a vector
of indices $\{a_i\}_{i=1}^M$ is generated such that $\sum_{i=1}^M
\bbI_{\{j\}}(a_i) = r_j$, and $1 \le a_i \le N$ for $i= 1,\dots,M$. And last,
set $\bar{X}^{(i)} = X^{(a_i)}$.

The first step determines the statistical properties of the resampling
algorithm. The library defines all algorithms discussed in
\textcite{Douc:2005wa}. Samplers can be constructed with builtin schemes as
seen in section~\ref{sub:Implementations}. In addition, samplers can also be
constructed with user defined resampling operations. A user defined resampling
algorithm can be any type that is convertible to
\verb|Sampler<T>::resample_type|,
following function call,
\begin{cppcode}
  using resample_type = std::function<void(std::size_t, std::size_t,
      typename Particle<T>::rng_type &, const double *, size_type *)>;
\end{cppcode}
where the first argument is $N$, the sample size before resampling; the second
is $M$, the sample size after resampling; the third is a \cppoo{} \rng type,
the fourth is a pointer to normalized weight, and the last is a pointer to the
vector $\{r_i\}_{i=1}^N$. The builtin schemes are implemented as classes with
\verb|operator()| conforms to the above signature. All builtin schemes are
listed in table~\ref{tab:Resampling schemes}

\begin{table}
  \begin{tabularx}{\textwidth}{lX}
    \toprule
    \verb|ResampleScheme| & Algorithm \\
    \midrule
    \verb|Multinomial|        & Multinomial resampling             \\
    \verb|Stratified|         & Stratified resampling              \\
    \verb|Systematic|         & Systematic resampling              \\
    \verb|Residual|           & Residual resampling                \\
    \verb|ResidualStratified| & Stratified resampling on residuals \\
    \verb|ResidualSystematic| & Systematic resampling on residuals \\
    \bottomrule
  \end{tabularx}
  \caption{Resampling schemes}
  \label{tab:Resampling schemes}
\end{table}

To transform $\{r_i\}_{i=1}^N$ into $\{a_i\}_{i=1}^M$, one can call the
following function,
\begin{cppcode}
  template <typename IntType1, typename IntType2>
  void resample_trans_rep_index(std::size_t N, std::size_t M,
      const IntType1 *replication, IntType2 *index);
\end{cppcode}
where the last parameter is the output vector $\{a_i\}_{i=1}^M$. This function
guarantees that $a_i = i$ if $r_i > 0$, for $i = 0,\dots,\min\{N, M\}$.
However, its output may not be optimal for all applications. The last step of a
resampling operation, the copying of particles can be the most time consuming
one, especially on distributed systems. The topology of the system will need to
be taking into consideration to achieve optimal performance. In those
situations, it is best to use \verb|ResampleMultinomial| etc., to generate the
replication numbers, and manually perform the rest of the resampling algorithm.

\section{User defined algorithms}
\label{sec:User defined algorithms}

The library provides facilities for implementing new resampling algorithms.
The most common situation is that, a vector of random numbers on the interval
$[0, 1)$ is generated, say $\{u_i\}_{i=1}^M$. The replication numbers are $r_i
= \sum_{j=1}^M \bbI_{[v_{i-1},v_i)}(u_i)$, where $v_i = \sum_{j=1}^i W_i$, $v_0
= 0$. For example, the Multinomial resampling algorithm is equivalent to
$\{u_i\}_{i=1}^M$ being i.i.d.\ standard uniform random numbers.

Alternatively, let $p_i = \Floor{MW_i}$, $q_i = MW_i - p_i$. One can perform
resampling on the residuals, using weights proportional to $\{q_i\}_{i=1}^N$.
The output size shall be $R = M - \sum_{i=1}^N p_i$. Let the replication
numbers be $\{s_i\}_{i=1}^N$, then $r_i = p_i + s_i$.

The library provides the following class template for implementing such
algorithms,
\begin{cppcode}
  template <template <typename, typename> class U01SeqType, bool Residual>
  class ResampleAlgorithm;
\end{cppcode}
where \verb|U01SeqType| will be discussed later. The second parameter
\verb|Residual| determines if the resampling shall be applied to residuals.

The template template parameter \verb|U01SeqType| shall be a class template
that takes the following form,
\begin{cppcode}
  template <typename RNGType, typename RealType>
  class U01SeqType
  {
        public:
        U01SeqType(std::size_t M, RNGType &rng);

        RealType operator[](std::size_t i);
  };
\end{cppcode}
The constructor and operator are required. The specific behavior of the
operator is more tricky. Let \verb|u01seq| be an object of the above type. The
operator \verb|u01seq[i]| will be called exactly $M$ times, with arguments $i =
1,\dots,M$, in the increasing order. Each time \verb|u01seq[i]| is called, it
shall return a number between \verb|u01seq[i-1]| and $1$. In other words, a
\verb|U01SeqType| object shall be able to generate ordered random numbers on
the interval $[0, 1)$. However, it only need to be able to generate such a
sequence once. After $M$ calls, or if its called twice with the same arguments,
or its called with a sequence of arguments other than $i=1,\dots,M$, its
behavior might well be undefined.

An obvious method is to generate the random numbers first and then sort them.
However, no sorting algorithm has cost $O(M)$, while it is possible to generate
such an ordered sequence with cost $O(M)$ by using order statistics. The
library defines three such sequences. The first is equivalent to sorted i.i.d.\
random numbers. The second and the third are stratified and systematic,
respectively. The builtin resampling algorithms are implemented using these
sequences. For example,
\begin{cppcode}
  using ResampleMultinomial = ResampleAlgorithm<U01SequenceSorted, false>;
  using ResampleResidual = ResampleAlgorithm<U01SequenceSorted, true>;
\end{cppcode}
If the user is able to define a new ordered random sequence, either through
sorting or otherwise, then using the \verb|ResampleAlgorithm| template, a new
resampling algorithm can easily be implemented. Note that, the algorithm
implemented by this class template always has a cost $O(N + M)$, unless the
random sequence has a greater cost.

\section{Algorithms with varying sample sizes}
\label{sec:Algorithms with varying sample sizes}

The library does not direct support varying sample size algorithms. However, it
is possible. Below is a program within which the sample size is changed through
a Multinomial resampling algorithm.
\begin{cppcode}
  // sampler is an existing Sampler<T> object
  auto N = sampler.size();
  auto &rng = sampler.particle().rng();
  auto weight = sampler.particle().weight().data();
  Vector<std::size_t> rep(N);
  Vector<std::size_t> idx(M);
  ResampleMultinomial resample;
  resample(N, M, rng, weight, rep.data());
  resample_trans_rep_index(N, M, rep.data(), idx.data());
  Particle<T> particle(M);
  particle.weight().set_equal();
  for (std::size_t i = 0; i != M; ++i) {
      auto sp_dst = particle.sp(i);
      auto sp_src = sampler.partice().sp(idx[i]);
      // Assuming T is a subclass of StateMatrix
      for (std::size_t d = 0; d != sp_dst.dim(); ++d)
          sp_dst.state(d) = sp_src.state(d);
  }
  // Copy other data of class T if any
  sampler.particle() = std::move(particle);
\end{cppcode}
The vectors of replication numbers \verb|rep| and indices \verb|idx| are
generated with the library's functions. Particles in the original sampler are
copied into a temporary particle system \verb|particle|. And then this
temporary is moved into the original sampler. It is important to note that, if
the \verb|T| type object \verb|particle.value()| has any other non-static
member data other than the states, they also need to be copied into the
temporary first. Note that \verb|sampler.size()| is only a shortcut for
\verb|sampler.particle().size()|. The sampler object itself does not store size
information, nor does it need to.

\section{Algorithms with increasing dimensions}
\label{sec:Algorithms with increasing dimensions}

Recall section~\ref{sec:Sequential importance sampling and resampling}, in
general an \sis algorithm operates on increasing dimensions. Assume that the
storage cost of a single particle at the marginal ($X_t^{(i)}$) is of order
$O(1)$. At each iterations $t$, the path $X_{0:t-1}^{(i)}$ is extended to
$X_{0:t}^{(i)}$. The resampling algorithms operate on the space
$\prod_{k=0}^tE_k$. When the proposal $q_t(\cdot|X_{0:t-1}^{(i)}) =
q_t(\cdot|X_{t-1}^{(i)})$ and only the marginal $\eta_t(X_t)$ is of interest,
one can only resample $\{X_t^{(i)}\}_{i=1}^N$. This leads to $O(N)$ cost for
resampling. This is the typical case for \smc algorithms. However, there are
situations where resampling $\{X_{0:t}^{(i)}\}_{i=1}^N$ is necessary. In this
case, the cost of resampling at iteration $t$ is $O(tN)$. And total resampling
cost to obtain $\{X_{0:t}^{(i)}\}_{i=1}^N$, is $O(t^2N)$.

However, such cost is avoidable in some circumstances. Recall that, after
generating the resampling index $\{a_i\}_{i=1}^N$, one set $\bar{X}_{0:t}^{(i)}
= X_{0:t}^{(a_i)}$. Let $(\{X_0^{(i)}\}_{i=1}^N,\dots,\{X_t^{(i)}\}_{i=1}^N)$
be the marginals before resampling, and
$(\{a_0^{(i)}\}_{i=1}^N,\dots,\{a_t^{(i)}\}_{i=1}^N)$ be the resampling index
vectors at each iteration. Then one can obtain $X_{0:t}^{(i)}$ through
the following recursion,
\begin{align*}
  b_t^{(i)} &= a_t^{(i)} & \\
  b_k^{(i)} &= a_{k}^{(b_{k + 1}^{(i)})} \text{ for } k = t - 1,\dots,0 \\
  \bar{X}_k^{(i)} &= X_k^{(b_k^{(i)})}   \text{ for } k = t,\dots,0\ \\
\end{align*}
Intuitively, only the resampling index vectors are resampled. The cost of the
above recursion is $O(tN)$ instead of $O(t^2N)$. Not that it is likely to be
much slower compared to directly copying particles at each iteration in the
situation when only the marginals need to be resampled, in which case both has
a cost $O(tN)$.

This algorithm is useful in the following situation. Assume that there exist
recursive functions,
\begin{align*}
\varphi_t(X_{0:t}^{(i)}|X_{0:t-1}^{(i)}) &=
\varphi_t(X_t^{(i)}, \varphi_{t-1}(X_{0:t-1}^{(i)})), \\
\phi_t(X_{0:t}^{(i)}|X_{0:t-1}^{(i)}) &=
\phi_t(X_t^{(i)}, \varphi_{t-1}(X_{0:t-1}^{(i)}), \phi_{t-1}(X_{0:t-1}^{(i)})),
\end{align*}
such that
$q(\cdot|X_{0:t}^{(i)}) = q(\cdot|\varphi(X_{0:t-1}^{(i)}))$ and
$W_t(X_{0:t}^{(i)}) = W_t(\phi(X_{0:t}^{(i)}))$. In this case, only the values
of $\varphi_t(X_{0:t}^{(i)})$ and $\phi_t(X_{0:t}^{(i)})$ need to be resampled
at every iteration. If they have storage costs at the order of $O(1)$, then the
total cost of resampling will still be $O(tN)$. See \textcite{stpf} for some
examples of such algorithms.

The library provides the following class template for implementing such an
algorithm.
\begin{cppcode}
template <typename IntType = std::size_t>
class ResampleIndex;
\end{cppcode}
Its usage is demonstrated by the following example (assuming
$X_t^{(i)}\in\Real$, and $\phi_t$ is the same as $\varphi_t$),
\begin{cppcode*}{texcomments}
  using StateBase = StateMatrix<vsmc::ColMajor, vsmc::Dynamic, double>;
  class State : StateBase
  {
      public:
      StateBase(std::size_t N) : StateBase(N), varphi_(N) {}

      template <typename IntType>
      void copy(std::size_t N, IntType *index)
      {
          // DO NOT CALL StateBase::copy
          for (std::size_t i = 0; i != N; ++i)
              varphi_[i] = varphi_[index[i]];
          index_.push_back(N, index);
      }

      // To be called during initialization
      void reset() { index_.reset(); }

      // Get the resampled state up to time n
      // Assuming that this->state(i, t) contains the marginal $X_t^{(i)}$
      State trace_back(std::size_t n)
      {
          // idxmat is an $N$ by $n + 1$ matrix, say $B$, such that
          // $B_{i,j} = b_j^{(i)}$
          auto idxmat = index_.index_matrix(ColMajor, n);
          State rs(*this);
          for (std::size_t j = 0; j <= n; ++j) {
              auto dst = rs.col_data(j);
              auto src = this->col_data(j);
              auto idx = idxmat.data() + j * this->size();
              for (std::size_t i = 0; i != this->size(); ++i)
                  dst[i] = src[idx[i]];
          }

          return rs;
      }

      private:
      Vector<double> varphi_; // the values of 
      ResampleIndex index_;
  };

  Sampler<State> sampler(N, vsmc::Multinomial); // Always resampling
  sampler.particle.value().resize_dim(n + 1);
  // configure the sampler
  sampler.initialize(param);
  sampler.iterate(n);
  auto state = sampler.particle().value().trace_back(n);
\end{cppcode*}
The method call \verb|index_.push_back(N, index)| append a new resampling index
vector to the history being recorded by \verb|index_|. If called without the
second argument, i.e., \verb|index_.push_back(N)|, then it is assumed $a_i = i$
for $i = 1,\dots,N$. To retrieve $b_{t_0}^{(i)}$, where $t_0 \le t$, one can
call \verb|index_.index(i, t, t0)|. If the last argument is omitted, it is
assumed to be zero. If the second argument is also omitted, then it is assumed
to be the iteration number of the last index vector recorded. It is of course
more useful, and more efficient to retrieve an $N$ by $R$ matrix $B$, such that
$R = t - t_0 + 1$, $B_{i,j} = b_j^{(i)}$. This is done by calling
\verb|index_.index_matrix(t, t0)|. Again, both arguments can be omitted, and
the default values are the same as for \verb|index_.index(i, t, t0)|.

The performance difference directly copy $X_{0:t}^{(i)}$ at each iteration and
using the above implementation can be significant for moderate to large $t$. Of
course, if $\eta_k(X_{0:t})$ is of interest for all $k \le t$, instead of only
$\eta_t(X_{0:t})$ being of interest, then one is better off to copy all states
at all iterations.

Note that, \verb|ResampleIndex| is capable of dealing with varying sample size
situations. Each call of \verb|push_back| does not need to have the same sample
size $N$. In addition, if such an index object need to be reused multiple
times, one can use its \verb|insert| method instead of \verb|push_back|. See
the reference manual for details.
